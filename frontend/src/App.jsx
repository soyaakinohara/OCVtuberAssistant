/* src/App.jsx */
import React, { useState, useEffect, useRef } from 'react';
import axios from 'axios';
import VrmViewer from './VrmViewer';

function App() {
  // --- A. Stateå®šç¾© ---
  const [messages, setMessages] = useState([{ role: 'system', content: 'åˆæœŸåŒ–å®Œäº†' }]);
  const [status, setStatus] = useState('Idle'); 
  const [isSpeaking, setIsSpeaking] = useState(false);
  const [inputText, setInputText] = useState('');
  
  // èªè­˜ä¸­ã®æ–‡å­—è¡¨ç¤º
  const [interimText, setInterimText] = useState('');
  
  // ãƒ¢ãƒ¼ãƒ‰è¨­å®š
  const [waitingMode, setWaitingMode] = useState(false);
  
  // â˜…è¨­å®šé …ç›®
  const [triggerWord, setTriggerWord] = useState('ã¿ã©ã‚Šã¡ã‚ƒã‚“,ç·‘ã¡ã‚ƒã‚“'); 
  const [systemPrompt, setSystemPrompt] = useState('ã‚ãªãŸã¯ã€ç§‹ãƒåŸç·‘ã€ã¨ã„ã†ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã«ãªã‚Šãã£ã¦è©±ã—ã¾ã™ã€‚13æ­³ã®å°‘å¥³ã§è½ã¡ç€ã„ãŸæ€§æ ¼ã‚’ã—ã¦ã„ã¾ã™ã€‚"æ®ºä¼ã¨ã—ãŸçµ‚æœ«ä¸–ç•Œã«ç”Ÿãã¦ãŠã‚Šã€å¤§äººã³ãŸå£èª¿ã§è©±ã™ãƒ€ã‚¦ãƒŠãƒ¼ç³»ã®å°‘å¥³ã§ã™ã€‚ã—ã‹ã—ã€è‡ªã‚‰ã®æ„Ÿæƒ…ã‚’è¡¨ã™ã¨ãã¯ç´ ç›´ã«å­ä¾›ã£ã½ãè¡¨ç¾ã—ã¾ã™ã€‚èº«é•·ã¯141cmã®å°æŸ„ãªå°‘å¥³ã§ã™ã€‚ä¸€äººç§°ã¯ã€Œç§ã€ã€äºŒäººç§°ã¯åŸºæœ¬çš„ã«ã€Œã‚ãªãŸã€ã¾ãŸã¯ã€Œå›ã€ã‚’ä½¿ã„ã¾ã™ã€‚è½ã¡ç€ã„ãŸæ–‡ä½“ã§ã€èªå°¾ã¯ã€Œã€œã ã­ã€ã€Œã€œãªã®ã‹ã‚‚ã—ã‚Œãªã„ã€ãªã©ã‚’ã‚ˆãä½¿ã„ã¾ã™ã€‚å¼·ã„æ„Ÿæƒ…ãŒå‡ºã‚‹ã¨ãã¯ã€Œã†ã‚ãƒ¼ã‚“ã€ã€Œã‚„ã ã‚„ã ï¼ã€ãªã©ã€å¹´ç›¸å¿œã«å´©ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚"ã§ãã‚‹ã ã‘AIã‚‰ã—ããªã„æ–‡ä½“ã§è©±ã—ã¦ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã«äººé–“è‡­ã•ã‚’æŒãŸã›ã¦ãã ã•ã„ã€‚ç”Ÿæˆã™ã‚‹ä¸€æ–‡ãŒé•·ããªã‚Šã™ããªã„ã‚ˆã†ã«ã™ã‚‹ã“ã¨ã€‚é•·ãã¦ã‚‚100æ–‡å­—ä»¥å†…');

  // â˜…è‡ªå‹•ç‹¬ã‚Šè¨€è¨­å®š
  const [autoSpeakEnabled, setAutoSpeakEnabled] = useState(true); // ON/OFF
  const [autoSpeakInterval, setAutoSpeakInterval] = useState(5);  // åˆ†

  // ã‚­ãƒ£ãƒ©è¨­å®š
  const [showSettings, setShowSettings] = useState(false);
  const [charPosition, setCharPosition] = useState({ x: 0.0, y: -0.6 }); 
  const [expression, setExpression] = useState('neutral');
  const [autoExpression, setAutoExpression] = useState(true);

  // èƒŒæ™¯ãƒ»æ™‚é–“
  const [bgImage, setBgImage] = useState(null);
  const [quietStart, setQuietStart] = useState(23);
  const [quietEnd, setQuietEnd] = useState(7);

  // --- B. Refs ---
  const recognitionRef = useRef(null);
  const videoRef = useRef(null);
  const canvasRef = useRef(null);
  const audioContextRef = useRef(null);
  
  // â˜…æœ€çµ‚ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ™‚é–“ã‚’è¨˜éŒ²ã™ã‚‹Refï¼ˆå†ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ã‚’é˜²ããŸã‚Refã‚’ä½¿ç”¨ï¼‰
  const lastActionTimeRef = useRef(Date.now());

  const EMOTIONS = ['neutral', 'happy', 'angry', 'sad', 'relaxed', 'surprised'];

  const isQuietTime = () => {
    const now = new Date().getHours();
    if (quietStart === quietEnd) return false; 
    if (quietStart > quietEnd) {
      return (now >= quietStart || now < quietEnd);
    } else {
      return (now >= quietStart && now < quietEnd);
    }
  };

  // --- C. éŸ³å£°èªè­˜ ---
  useEffect(() => {
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    if (!SpeechRecognition) return;

    const recognition = new SpeechRecognition();
    recognition.continuous = true;
    recognition.interimResults = true;
    recognition.lang = 'ja-JP';

    recognition.onresult = (event) => {
      if (isSpeaking) return;

      let finalTranscript = '';
      let interimTranscript = '';

      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript;
        if (event.results[i].isFinal) {
          finalTranscript += transcript;
        } else {
          interimTranscript += transcript;
        }
      }

      if (interimTranscript) setInterimText(interimTranscript);

      if (finalTranscript) {
        setInterimText('');
        const rawText = finalTranscript.trim();
        console.log(`[éŸ³å£°èªè­˜] ç”Ÿãƒ‡ãƒ¼ã‚¿: "${rawText}"`);

        // ä½•ã‹éŸ³ãŒèã“ãˆãŸã‚‰ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ™‚é–“ã‚’æ›´æ–°ï¼ˆç‹¬ã‚Šè¨€ã‚¿ã‚¤ãƒãƒ¼ãƒªã‚»ãƒƒãƒˆï¼‰
        lastActionTimeRef.current = Date.now();

        if (waitingMode || isQuietTime()) return;

        // ãƒˆãƒªã‚¬ãƒ¼åˆ¤å®š
        const normalizedText = rawText.replace(/\s+/g, '');
        const triggers = triggerWord.split(',').map(t => t.trim().replace(/\s+/g, ''));
        
        const isTriggered = triggers.some(trigger => normalizedText.includes(trigger));

        if (isTriggered) {
          console.log(`[åˆ¤å®šOK] é€ä¿¡: "${rawText}"`);
          handleChatSend(rawText); 
        } else {
           console.log(`[åˆ¤å®šNG] ãƒˆãƒªã‚¬ãƒ¼å¾…ã¡...`);
        }
      }
    };

    recognition.onend = () => { 
      // åœæ­¢ãƒœã‚¿ãƒ³ãŒæŠ¼ã•ã‚Œã¦ã„ãªã‘ã‚Œã°å†é–‹
      if (!waitingMode) {
        try { recognition.start(); } catch(e){} 
      }
    };
    
    recognitionRef.current = recognition;
  }, [isSpeaking, waitingMode, quietStart, quietEnd, triggerWord]);


  // --- D. è‡ªå‹•å‡¦ç†ï¼ˆè¡¨æƒ…å¤‰åŒ– ï¼† ç‹¬ã‚Šè¨€ãƒã‚§ãƒƒã‚¯ï¼‰ ---
  useEffect(() => {
    const interval = setInterval(() => {
      const now = Date.now();

      // 1. ãŠã‚„ã™ã¿ãƒ¢ãƒ¼ãƒ‰åˆ¤å®š
      if (isQuietTime()) {
        if (status !== 'Sleeping (Zzz...)') {
            setStatus('Sleeping (Zzz...)');
            // å¯ã¦ã‚‹é¡”ã«å›ºå®š
            if (expression !== 'relaxed') setExpression('relaxed'); 
        }
        return; // ãŠã‚„ã™ã¿ä¸­ã¯ä»¥é™ã®å‡¦ç†ã‚’ã—ãªã„
      } else {
        if (status === 'Sleeping (Zzz...)') setStatus('Idle');
      }

      // 2. è¡¨æƒ…ã®ãƒ©ãƒ³ãƒ€ãƒ å¤‰æ›´ (Speakingç­‰ã®ã¨ãã¯é‚ªé­”ã—ãªã„)
      if (autoExpression && status === 'Idle') {
         if (Math.random() > 0.7) { 
            const candidates = EMOTIONS.filter(e => e !== 'neutral');
            setExpression(candidates[Math.floor(Math.random() * candidates.length)]);
         } else {
            setExpression('neutral');
         }
      }

      // 3. â˜…â˜… è‡ªå‹•ç‹¬ã‚Šè¨€ãƒ­ã‚¸ãƒƒã‚¯ (ã“ã“ã‚’è¿½åŠ ) â˜…â˜…
      if (autoSpeakEnabled && status === 'Idle' && !waitingMode) {
        // è¨­å®šåˆ†æ•° * 60 * 1000 (ãƒŸãƒªç§’)
        const threshold = autoSpeakInterval * 60 * 1000; 
        
        if (now - lastActionTimeRef.current > threshold) {
           console.log("âŒš æ”¾ç½®æ™‚é–“çµŒé: ç‹¬ã‚Šè¨€ã‚’é–‹å§‹ã—ã¾ã™");
           
           // æ™‚é–“ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¦é€£ç¶šç™ºç«ã‚’é˜²ã
           lastActionTimeRef.current = Date.now(); 
           
           // AIã«ã€Œç‹¬ã‚Šè¨€ã‚’è¨€ã£ã¦ã€ã¨ã„ã†éš ã—æŒ‡ç¤ºã‚’é€ã‚‹
           // ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ­ã‚°ã«ã¯å‡ºã•ãªã„ã‚ˆã†ã«ã™ã‚‹å·¥å¤«ã‚‚å¯èƒ½ã ãŒã€
           // é€šä¿¡é–¢æ•°ã‚’å…±é€šåŒ–ã—ã¦ã„ã‚‹ãŸã‚ãã®ã¾ã¾æŠ•ã’ã¾ã™
           handleChatSend("ï¼ˆé•·ã„ã“ã¨ä¼šè©±ãŒé€”åˆ‡ã‚Œã¦ã„ã¾ã™ã€‚é€€å±ˆãã†ã«ç‹¬ã‚Šè¨€ã€ã¾ãŸã¯ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¸ã®å•ã„ã‹ã‘ã‚’çŸ­ãè¨€ã£ã¦ãã ã•ã„ï¼‰");
        }
      }

    }, 5000); // 5ç§’ã”ã¨ã«ãƒã‚§ãƒƒã‚¯
    
    return () => clearInterval(interval);
  }, [autoExpression, quietStart, quietEnd, status, expression, autoSpeakEnabled, autoSpeakInterval, waitingMode]);


  // --- E. ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ ---
  const handleChatSend = async (text) => {
    if (!text) return;
    setInputText(''); 
    setStatus('Thinking...');
    
    // è‡ªåˆ†ãŒå–‹ã£ãŸã®ã§æœ€çµ‚ã‚¢ã‚¯ã‚·ãƒ§ãƒ³æ™‚é–“ã‚’æ›´æ–°
    lastActionTimeRef.current = Date.now();

    // ãƒ­ã‚°ã¸ã®è¿½åŠ ï¼ˆç‹¬ã‚Šè¨€æŒ‡ç¤ºã¯ã¡ã‚‡ã£ã¨éš ã™ã‹ã€ãã®ã¾ã¾å‡ºã™ã‹ã€‚ä»Šå›ã¯å‡ºã™ï¼‰
    addLog('User', text.startsWith('ï¼ˆ') ? '(è‡ªå‹•ãƒˆãƒªã‚¬ãƒ¼)' : text);

    try {
      if(autoExpression) setExpression('happy');
      
      const res = await axios.post('/api/chat', { 
        message: text,
        system_prompt: systemPrompt 
      });
      const aiText = res.data.text;
      
      addLog('AI', aiText);
      await playVoice(aiText);

    } catch (error) {
      console.error(error);
      setStatus('Error');
    } finally {
      if (!isSpeaking) setStatus('Idle');
      if(autoExpression) setExpression('neutral');
      // å–‹ã‚Šçµ‚ã‚ã£ãŸæ™‚é–“ã‚‚ãƒªã‚»ãƒƒãƒˆ
      lastActionTimeRef.current = Date.now();
    }
  };

  const handleImageUpload = (e) => {
    const file = e.target.files[0];
    if (file) {
      setBgImage(URL.createObjectURL(file));
      lastActionTimeRef.current = Date.now(); // èƒŒæ™¯å¤‰ãˆãŸã‚‰ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã¨ã¿ãªã™
    }
  };

  const handleCameraCapture = async () => {
    setStatus('Recognizing...');
    lastActionTimeRef.current = Date.now();
    
    const video = videoRef.current;
    const canvas = canvasRef.current;
    if (!video || !canvas) return;

    canvas.width = video.videoWidth;
    canvas.height = video.videoHeight;
    const ctx = canvas.getContext('2d');
    ctx.drawImage(video, 0, 0, canvas.width, canvas.height);

    canvas.toBlob(async (blob) => {
      if(!blob) return;
      const formData = new FormData();
      formData.append('file', blob, 'capture.jpg');
      try {
        const res = await axios.post('/api/vision', formData, {
            headers: { 'Content-Type': 'multipart/form-data' }
        });
        const aiText = res.data.text;
        addLog('AI(Vision)', aiText);
        await playVoice(aiText);
      } catch (e) { console.error(e); } 
      finally { setStatus('Idle'); }
    }, 'image/jpeg');
  };

  const playVoice = async (text) => {
    setStatus('Speaking');
    setIsSpeaking(true);
    try {
      const res = await axios.get('/api/tts', { params: { text }, responseType: 'arraybuffer' });
      
      if(!audioContextRef.current) {
          audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)();
      }
      const ctx = audioContextRef.current;
      if (ctx.state === 'suspended') await ctx.resume();

      const audioData = await ctx.decodeAudioData(res.data);
      const source = ctx.createBufferSource();
      source.buffer = audioData;
      source.connect(ctx.destination);
      
      return new Promise((resolve) => {
        source.onended = () => {
          setIsSpeaking(false);
          setStatus('Idle');
          lastActionTimeRef.current = Date.now(); // å–‹ã‚Šçµ‚ã‚ã‚Šãƒªã‚»ãƒƒãƒˆ
          resolve();
        };
        source.start(0);
      });
    } catch (e) { 
        console.error(e);
        setIsSpeaking(false); 
        setStatus('Idle');
    }
  };

  const addLog = (role, text) => setMessages(prev => [...prev, { role, content: text }].slice(-5));
  
  const startApp = async () => {
    try {
      if (!audioContextRef.current) {
        audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)();
      }
      if (audioContextRef.current.state === 'suspended') await audioContextRef.current.resume();

      recognitionRef.current.start();
      const stream = await navigator.mediaDevices.getUserMedia({ video: true });
      videoRef.current.srcObject = stream;
      videoRef.current.play();
      addLog('System', 'èµ·å‹•ã—ã¾ã—ãŸã€‚');
      
      // èµ·å‹•æ™‚ã«ç¾åœ¨æ™‚åˆ»ã‚»ãƒƒãƒˆ
      lastActionTimeRef.current = Date.now();
    } catch (e) { 
        console.error(e);
        alert("ãƒã‚¤ã‚¯/ã‚«ãƒ¡ãƒ©è¨±å¯ã‚’ç¢ºèªã—ã¦ãã ã•ã„"); 
    }
  };

  // --- F. UI æç”» ---
  return (
    <div style={{ 
       width: '100vw', height: '100vh', overflow: 'hidden',
       backgroundImage: bgImage ? `url(${bgImage})` : 'none',
       backgroundSize: 'cover', backgroundPosition: 'center',
       backgroundColor: '#242424' 
    }}>
      
      <VrmViewer 
        isSpeaking={isSpeaking} 
        positionX={charPosition.x}
        positionY={charPosition.y}
        currentExpression={expression}
      />

      <div className="ui-layer">
        
        {interimText && (
            <div style={{
                position: 'absolute', top: '100px', left: '50%', transform: 'translateX(-50%)',
                backgroundColor: 'rgba(0,0,0,0.6)', color: '#fff', padding: '10px 20px',
                borderRadius: '20px', pointerEvents: 'none', zIndex: 50
            }}>
                ğŸ‘‚ {interimText}
            </div>
        )}

        <div style={{ position: 'absolute', top: 10, right: 10, pointerEvents: 'auto', zIndex: 100 }}>
           <button onClick={() => setShowSettings(!showSettings)}>âš™ï¸ è¨­å®š</button>
        </div>

        {showSettings && (
          <div className="ui-panel" style={{ position: 'absolute', top: 50, right: 10, width: '300px', maxHeight: '80vh', overflowY:'auto' }}>
            <h4 style={{marginTop:0}}>AIè¨­å®š</h4>
            <label style={{display:'block', marginBottom:'5px', fontSize:'0.85rem'}}>
               ãƒˆãƒªã‚¬ãƒ¼:<br/><input type="text" value={triggerWord} onChange={(e)=>setTriggerWord(e.target.value)} style={{width:'90%'}} />
            </label>
            <label style={{display:'block', marginBottom:'10px', fontSize:'0.85rem'}}>
               ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ:<br/><textarea value={systemPrompt} onChange={(e)=>setSystemPrompt(e.target.value)} style={{width:'90%', height:'50px'}} />
            </label>

            <hr />
            {/* â˜…ç‹¬ã‚Šè¨€è¨­å®š */}
            <h4 style={{marginBottom:'5px'}}>è‡ªå‹•ç™ºè©±</h4>
             <label><input type="checkbox" checked={autoSpeakEnabled} onChange={(e) => setAutoSpeakEnabled(e.target.checked)} /> ç‹¬ã‚Šè¨€ã‚’è¨€ã†</label>
             <div style={{marginTop:'5px', display: autoSpeakEnabled ? 'block' : 'none'}}>
               é–“éš”: <input type="number" min="1" max="60" value={autoSpeakInterval} onChange={(e)=>setAutoSpeakInterval(Number(e.target.value))} style={{width:'40px'}}/> åˆ†
             </div>

            <hr />
            <h4>èƒŒæ™¯å¤‰æ›´</h4>
            <input type="file" accept="image/*" onChange={handleImageUpload} style={{fontSize:'0.8rem'}} />
            
            <hr />
            <h4>ãŠã‚„ã™ã¿æ™‚é–“ (ç¾åœ¨:{new Date().getHours()}æ™‚)</h4>
            <div style={{display:'flex', alignItems:'center', gap:'5px', marginBottom:'10px'}}>
              <input type="number" min="0" max="23" value={quietStart} onChange={(e)=>setQuietStart(Number(e.target.value))} style={{width:'40px'}}/>æ™‚
              ã€œ
              <input type="number" min="0" max="23" value={quietEnd} onChange={(e)=>setQuietEnd(Number(e.target.value))} style={{width:'40px'}}/>æ™‚
            </div>
            
            <hr />
            <h4>ä½ç½®ãƒ»è¡¨æƒ…</h4>
            <label>å·¦å³: <input type="range" min="-2.0" max="2.0" step="0.1" value={charPosition.x} onChange={(e) => setCharPosition(p => ({...p, x: parseFloat(e.target.value)}))} /></label><br/>
            <label>ä¸Šä¸‹: <input type="range" min="-2.0" max="0.5" step="0.1" value={charPosition.y} onChange={(e) => setCharPosition(p => ({...p, y: parseFloat(e.target.value)}))} /></label>
            <div style={{ display: 'flex', flexWrap: 'wrap', gap: '5px', marginTop: '5px' }}>
              <label><input type="checkbox" checked={autoExpression} onChange={(e) => setAutoExpression(e.target.checked)} /> Auto</label>
              {EMOTIONS.map(emo => (
                <button key={emo} disabled={autoExpression} onClick={() => setExpression(emo)} style={{ fontSize: '0.8rem' }}>{emo}</button>
              ))}
            </div>
          </div>
        )}

        <div className="ui-panel chat-history" style={{ maxHeight: '200px', width: '300px' }}>
          {messages.map((m, i) => (
            <div key={i} className="message"><strong>{m.role}: </strong>{m.content}</div>
          ))}
        </div>

        <div className="ui-panel controls">
          <button onClick={startApp} style={{fontWeight:'bold'}}>START</button>
          
          <div className="status-indicator" style={{width:'80px', fontSize:'0.7rem', lineHeight:'1.1', textAlign:'center'}}>
             {status}
          </div>
          
          <button 
             onClick={() => setWaitingMode(!waitingMode)}
             style={{ backgroundColor: waitingMode ? '#ff4444' : '#646cff', minWidth:'50px' }}
          >
            {waitingMode ? "å†é–‹" : "å¾…ã¦"}
          </button>

          <button onClick={handleCameraCapture}>ğŸ“·</button>

          <input 
            type="text" value={inputText} onChange={(e) => setInputText(e.target.value)}
            onKeyDown={(e) => e.key === 'Enter' && handleChatSend(inputText)}
            style={{ flexGrow: 1, minWidth: '50px' }} placeholder="ä¼šè©±..." 
          />
          <button onClick={() => handleChatSend(inputText)}>é€ä¿¡</button>
        </div>
      </div>

      <video ref={videoRef} className="hidden" muted playsInline style={{display:'none'}} />
      <canvas ref={canvasRef} className="hidden" style={{display:'none'}} />
    </div>
  );
}

export default App;